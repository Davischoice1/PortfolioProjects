{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Step 1: Setting Up the Environment\n","\n","Story:\n","\n","You start by setting up your development environment. You decide to use FastAPI for building the REST API because of its speed and ease of use. You also plan to use Docker to containerize the application for easy deployment and scalability. Finally, you set up Kafka to handle real-time streaming of transaction data for live predictions.\n","\n","Mini Tasks:\n","\n","\n","Install Python and create a virtual environment.\n","Install FastAPI, Uvicorn, and other required libraries (e.g., pydantic, scikit-learn, kafka-python).\n","Set up a local Kafka server using Docker (use the confluentinc/cp-kafka image).\n","Verify that Kafka is running by creating a test topic and producing/consuming messages.\n"],"metadata":{"id":"aUqEp2tAd4XJ"}},{"cell_type":"code","source":[" # Install Python (3.10+) and create a virtual environment:\n","\n","  ```bash\n","  python -m venv venv\n","  source venv/bin/activate  # or venv\\Scripts\\activate on Windows\n","  ```\n","# Install required Python libraries:\n","\n","  ```bash\n","  pip install fastapi uvicorn[standard] pydantic scikit-learn kafka-python prometheus-fastapi-instrumentator joblib requests pandas\n","  ```\n","# Set up Kafka using Docker:\n","\n","  ```yaml\n","  # Included in docker-compose.yml (see below)\n","  - Uses `confluentinc/cp-zookeeper` and `confluentinc/cp-kafka`\n","  ```\n","# Verify Kafka setup:\n","\n","  ```bash\n","  docker-compose exec kafka bash\n","  kafka-topics --bootstrap-server localhost:9092 --create --topic test-topic --partitions 1 --replication-factor 1\n","  kafka-console-producer --bootstrap-server localhost:9092 --topic test-topic\n","  kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning"],"metadata":{"id":"4a_unO4U6EfS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Building the Machine Learning Model\n","\n","Story:\n","\n","You already have a trained fraud detection model, but for this project, you decide to train a simple logistic regression model on a sample dataset (e.g., the Credit Card Fraud Detection dataset). You save the trained model as a .pkl file for later use.\n","\n","Mini Tasks:\n","\n","\n","Load the dataset and preprocess it (e.g., handle missing values, scale features).\n","Train a logistic regression model using scikit-learn.\n","Save the trained model as a .pkl file using joblib or pickle."],"metadata":{"id":"rOJ6YHNLeMgD"}},{"cell_type":"code","source":["\n","import kagglehub\n","\n","# # Download latest version\n","path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n","\n","print(\"Path to dataset files:\", path)\n","\n","import pandas as pd\n","df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n","df.head()\n","\n","\"\"\"## Exploring Data\"\"\"\n","\n","df.shape\n","\n","df.info()\n","\n","df.describe()\n","\n","\"\"\"## Checking for Missing Values\"\"\"\n","\n","df.isnull().sum()\n","\n","\"\"\"## Handling duplicates\"\"\"\n","\n","duplicate_values = df.duplicated().sum()\n","print(f'Number of duplicate rows: {duplicate_values}')\n","\n","df = df.drop_duplicates()\n","print(f'Number of rows after dropping duplicates: {df.shape[0]}')\n","\n","\"\"\"## Checking Dataset is Balanced or Imbalanced\"\"\"\n","\n","classes=df['Class'].value_counts()\n","print(f'normal_trans ={classes[0]}')\n","print(f'fraud_trans ={classes[1]}')\n","print(f'percentage_normal_trans ={(classes[0] / df[\"Class\"].count())*100:.2f}%')\n","print(f'percentage_fraud_trans ={(classes[1] / df[\"Class\"].count())*100:.2f}%')\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","title=['normal_trans','fraud_trans']\n","value=[classes[0],classes[1]]\n","plt.figure(figsize=(5, 5))\n","bars = plt.bar(title, value, color='lightblue')\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width() / 2, yval + 20,\n","             str(int(yval)),\n","             ha='center', va='bottom', fontweight='bold')\n","plt.title('Class Distribution')\n","plt.xlabel('Transaction Type')\n","plt.ylabel('Count')\n","plt.show()\n","\n","\"\"\"## Exploratory Data Analysis\"\"\"\n","\n","\n","\n","\"\"\"### Correlation Matrix\n","\n","Let's look at the correlation matrix to see how the features are related, especially with the 'Class' variable.\n","\"\"\"\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(df.corr(), cmap='coolwarm', annot=False)\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","print(df.columns)\n","\n","x = df.drop('Class', axis=1)#features\n","y=df['Class']#target\n","var=x.columns\n","fig, axes = plt.subplots(10, 3, figsize=(30, 45))\n","axes = axes.flatten()\n","\n","for i, ax in enumerate(axes):\n","    sns.histplot(x[var[i]], ax=ax)\n","    ax.set_title(var[i])\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(x.columns)\n","print(x.shape)\n","\n","\"\"\"### Skewness\n","When plotting the features, we observe that not all of them follow a normal distribution. To assess the skewness, we will use the skew() function. If any feature has a skew value, we will apply a PowerTransformer, which helps reduce skewness and transforms the data to follow a more normal distribution.\n","\"\"\"\n","\n","skewness=x.skew()\n","skew=[]\n","for i in var:\n","  skew.append(x[i].skew())\n","skew_df = pd.DataFrame({'Features': var, 'Skewness': skew})\n","print(skew_df)\n","\n","from sklearn.preprocessing import PowerTransformer\n","skewed_columns = skew_df.loc[abs(skew_df['Skewness']) > 1, 'Features']\n","print(\"Highly skewed columns:\", list(skewed_columns))\n","pt = PowerTransformer(method='yeo-johnson', copy=False)\n","x[skewed_columns] = pt.fit_transform(x[skewed_columns])\n","\n","\"\"\"### Scatter Plot of Time vs. Amount by Class\n","\n","Let's create a scatter plot to visualize the relationship between 'Time' and 'Amount' for both normal and fraudulent transactions.\n","\"\"\"\n","\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(data=df, x='Time', y='Amount', hue='Class', alpha=0.6)\n","plt.title('Scatter Plot of Time vs. Amount by Class')\n","plt.xlabel('Time')\n","plt.ylabel('Amount')\n","plt.show()\n","\n","\"\"\"## Outliers\n","In this dataset, there may be instances where the card is used for high-value transactions, which is why we cannot simply remove outliers.\n","\n","## Normalization\n","\"\"\"\n","\n","from sklearn.preprocessing import RobustScaler\n","scaler=RobustScaler()\n","x[['Amount']]=scaler.fit_transform(x[['Amount']])\n","x.head()\n","\n","\"\"\"## SMOTE\n","My dataset was highly imbalanced, so I applied SMOTE to balance the data.\n","\"\"\"\n","\n","from imblearn.over_sampling import SMOTE\n","from collections import Counter\n","\n","print(\"Before SMOTE:\", Counter(y))\n","\n","smote = SMOTE(sampling_strategy=0.1, random_state=42)\n","X_resampled, y_resampled = smote.fit_resample(x, y)\n","\n","print(\"After SMOTE:\", Counter(y_resampled))\n","\n","\"\"\"## StratifiedKFold\n","For imbalanced datasets, I use StratifiedKFold to ensure equal representation of all classes in each fold.\n","\"\"\"\n","\n","from sklearn.model_selection import StratifiedKFold\n","\n","skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n","\n","\"\"\"### Plotting Classification Metrics and Confusion Matrix\n","\n","Now, let's visualize the performance of the Random Forest model by plotting the accuracy, ROC AUC, and the confusion matrix.\n","\"\"\"\n","\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, roc_auc_score,\n","    confusion_matrix, ConfusionMatrixDisplay\n",")\n","from sklearn.inspection import PartialDependenceDisplay\n","import matplotlib.pyplot as plt\n","import joblib\n","\n","# Stratified K-Fold\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# ====== PIPELINE + GRID SEARCH (Feature selection + scaling + logistic regression) ====== #\n","pipeline = Pipeline([\n","    (\"scaler\", StandardScaler()),\n","    (\"feature_selection\", SelectKBest(score_func=f_classif, k='all')),\n","    (\"logreg\", LogisticRegression(random_state=42, solver='liblinear'))\n","])\n","\n","param_grid = {\n","    'logreg__C': [0.01, 0.1, 1, 10, 100],\n","    'logreg__penalty': ['l1', 'l2']\n","}\n","\n","grid_search = GridSearchCV(pipeline, param_grid, cv=skf, scoring='roc_auc', n_jobs=-1)\n","grid_search.fit(X_resampled, y_resampled)\n","\n","best_model = grid_search.best_estimator_\n","print(\"‚úÖ Best Hyperparameters:\", grid_search.best_params_)\n","\n","# ====== EVALUATE ON CROSS-VALIDATION ====== #\n","print(f\"\\nüîç Cross-validating Best Logistic Regression Model\")\n","accuracies, precisions, roc_aucs = [], [], []\n","\n","for train_idx, val_idx in skf.split(X_resampled, y_resampled):\n","    X_train, X_val = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]\n","    y_train, y_val = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]\n","\n","    best_model.fit(X_train, y_train)\n","    y_pred = best_model.predict(X_val)\n","    y_proba = best_model.predict_proba(X_val)[:, 1]\n","\n","    accuracies.append(accuracy_score(y_val, y_pred))\n","    precisions.append(precision_score(y_val, y_pred))\n","    roc_aucs.append(roc_auc_score(y_val, y_proba))\n","\n","# Print average metrics\n","print(\"===============================================\")\n","print(f\"üìä Mean Accuracy:  {np.mean(accuracies):.4f}\")\n","print(f\"üìä Mean Precision: {np.mean(precisions):.4f}\")\n","print(f\"üìä Mean ROC AUC:   {np.mean(roc_aucs):.4f}\")\n","print(\"===============================================\")\n","\n","# ====== Save Model ====== #\n","filename = 'logistic_regression_model.joblib'\n","joblib.dump(best_model, filename)\n","print(f\"üì¶ Model saved as: {filename}\")\n","\n","# ========== PLOT: Accuracy & ROC AUC ========== #\n","# Define the accuracy and ROC AUC values from the previous cell's output\n","accuracy_plot = np.mean(accuracies)\n","roc_auc_plot = np.mean(roc_aucs)\n","\n","plt.figure(figsize=(8, 5))\n","metrics_names = ['Accuracy', 'ROC AUC']\n","metrics_values = [accuracy_plot, roc_auc_plot]\n","colors = ['skyblue', 'lightgreen']\n","\n","bars = plt.bar(metrics_names, metrics_values, color=colors)\n","plt.ylabel('Score')\n","plt.title('Logistic Regression Model Performance (Test Set)')\n","plt.ylim(0, 1)\n","\n","for bar in bars:\n","    yval = bar.get_height()\n","    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{yval:.4f}\", ha='center', va='bottom')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# ====== Confusion Matrix on Test Split ====== #\n","X_train_plot, X_test_plot, y_train_plot, y_test_plot = train_test_split(\n","    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n",")\n","\n","best_model.fit(X_train_plot, y_train_plot)\n","y_pred_test = best_model.predict(X_test_plot)\n","cm = confusion_matrix(y_test_plot, y_pred_test)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n","\n","plt.figure(figsize=(6, 6))\n","disp.plot(cmap=plt.cm.Blues)\n","plt.title('Confusion Matrix: Logistic Regression (Test Set)')\n","plt.show()\n","\n","# ====== Partial Dependence Plot ====== #\n","print(\"üìà Partial Dependence Plot: Logistic Regression\")\n","features_to_plot = [0, 1, 2]  # Adjust these based on your dataset\n","PartialDependenceDisplay.from_estimator(best_model, X_resampled, features=features_to_plot)\n","plt.suptitle(\"Logistic Regression - Partial Dependence\", y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","\n","\n"],"metadata":{"id":"QpAzkwHQev7p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3: Creating the REST API with FastAPI**\n","\n","**Story: **\n","\n","You build a REST API using FastAPI that exposes an endpoint for making predictions. The API takes transaction data as input, loads the trained model, and returns the prediction (fraudulent or not).\n","\n","**Mini Tasks:**\n","\n","\n","Create a FastAPI application with a /predict endpoint.\n","Load the trained model from the .pkl file when the API starts.\n","Define a Pydantic model for the input data (e.g., transaction amount, timestamp, features).\n","Implement the prediction logic in the /predict endpoint.\n","Test the API locally using Uvicorn and sample transaction data."],"metadata":{"id":"bq2uSzRxgGrz"}},{"cell_type":"markdown","source":["**Fastapi(main.py)**"],"metadata":{"id":"85cuzpFEg7Rz"}},{"cell_type":"code","source":["from fastapi import FastAPI, HTTPException, Request\n","from pydantic import BaseModel\n","import pandas as pd\n","import joblib\n","import logging\n","import os\n","\n","from prometheus_fastapi_instrumentator import Instrumentator\n","\n","# Setup logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Initialize FastAPI app\n","app = FastAPI(\n","    title=\"Fraud Detection API\",\n","    description=\"API that predicts whether a transaction is fraudulent or legitimate.\",\n","    version=\"1.0.0\"\n",")\n","\n","# Instrumentator for Prometheus metrics\n","Instrumentator().instrument(app).expose(app)\n","\n","# Define the expected feature names\n","FEATURE_NAMES = [\n","    \"Time\", \"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\", \"V9\",\n","    \"V10\", \"V11\", \"V12\", \"V13\", \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\",\n","    \"V20\", \"V21\", \"V22\", \"V23\", \"V24\", \"V25\", \"V26\", \"V27\", \"V28\", \"Amount\"\n","]\n","\n","# Define the Pydantic model\n","class TransactionInput(BaseModel):\n","    Time: float\n","    V1: float\n","    V2: float\n","    V3: float\n","    V4: float\n","    V5: float\n","    V6: float\n","    V7: float\n","    V8: float\n","    V9: float\n","    V10: float\n","    V11: float\n","    V12: float\n","    V13: float\n","    V14: float\n","    V15: float\n","    V16: float\n","    V17: float\n","    V18: float\n","    V19: float\n","    V20: float\n","    V21: float\n","    V22: float\n","    V23: float\n","    V24: float\n","    V25: float\n","    V26: float\n","    V27: float\n","    V28: float\n","    Amount: float\n","\n","# Load model\n","MODEL_PATH = os.getenv(\"MODEL_PATH\", \"app/model/logistic_regression_model.joblib\")\n","\n","if not os.path.exists(MODEL_PATH):\n","    logger.error(f\"Model file not found at {MODEL_PATH}\")\n","    raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n","\n","try:\n","    model = joblib.load(MODEL_PATH)\n","    logger.info(\"Model loaded successfully.\")\n","except Exception as e:\n","    logger.error(f\"Error loading model: {e}\")\n","    raise RuntimeError(f\"Could not load model: {e}\")\n","\n","# Root endpoint\n","@app.get(\"/\")\n","def read_root():\n","    return {\"message\": \"Welcome! This API enables real-time fraud detection for financial transactions.\"}\n","\n","# Health check endpoint\n","@app.get(\"/health\")\n","def health_check():\n","    return {\"status\": \"ok\"}\n","\n","# Prediction endpoint\n","@app.post(\"/predict\")\n","def predict(transaction: TransactionInput, request: Request):\n","    logger.info(f\"Received prediction request from {request.client.host}\")\n","\n","    try:\n","        # Convert the input data to a DataFrame\n","        input_data = pd.DataFrame(\n","            [[getattr(transaction, col) for col in FEATURE_NAMES]],\n","            columns=FEATURE_NAMES\n","        )\n","        # Make prediction\n","        prediction = model.predict(input_data)[0]\n","        probability = model.predict_proba(input_data)[0][1]\n","        result = \"Fraudulent\" if prediction == 1 else \"Legitimate\"\n","\n","        logger.info(f\"Prediction: {result}, Probability: {probability:.4f}\")\n","\n","        return {\n","            \"prediction\": result,\n","            \"fraud_probability\": round(probability, 4)\n","        }\n","\n","    except Exception as e:\n","        logger.error(f\"Prediction failed: {e}\")\n","        raise HTTPException(status_code=500, detail=\"Prediction failed. Check input data.\")\n"],"metadata":{"id":"CzNTpTqogZfb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**kafka_worker.py**"],"metadata":{"id":"R-dzFEvZ89CE"}},{"cell_type":"code","source":["from kafka import KafkaConsumer, KafkaProducer\n","import json\n","import logging\n","import pandas as pd\n","import joblib\n","import os\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(\"KafkaWorker\")\n","\n","KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"kafka:9092\")\n","TRANSACTIONS_TOPIC = \"transactions\"\n","PREDICTIONS_TOPIC = \"predictions\"\n","\n","MODEL_PATH = os.getenv(\"MODEL_PATH\", \"app/model/logistic_regression_model.joblib\")\n","FEATURE_NAMES = [\"Time\"] + [f\"V{i}\" for i in range(1, 29)] + [\"Amount\"]\n","\n","try:\n","    model = joblib.load(MODEL_PATH)\n","    logger.info(\"‚úÖ Model loaded.\")\n","except Exception as e:\n","    logger.error(f\"‚ùå Failed to load model: {e}\")\n","    raise\n","\n","def predict(features_dict):\n","    df = pd.DataFrame([[features_dict[col] for col in FEATURE_NAMES]], columns=FEATURE_NAMES)\n","    prediction = model.predict(df)[0]\n","    probability = model.predict_proba(df)[0][1]\n","    result = {\n","        \"prediction\": \"Fraudulent\" if prediction == 1 else \"Legitimate\",\n","        \"fraud_probability\": round(probability, 4)\n","    }\n","    return result\n","\n","consumer = KafkaConsumer(\n","    TRANSACTIONS_TOPIC,\n","    bootstrap_servers=KAFKA_BROKER,\n","    value_deserializer=lambda m: json.loads(m.decode(\"utf-8\")),\n","    auto_offset_reset=\"earliest\",\n","    group_id=\"fraud-detector\"\n",")\n","\n","producer = KafkaProducer(\n","    bootstrap_servers=KAFKA_BROKER,\n","    value_serializer=lambda m: json.dumps(m).encode(\"utf-8\")\n",")\n","\n","logger.info(\"üîÅ Kafka worker is now listening...\")\n","\n","try:\n","    for msg in consumer:\n","        data = msg.value\n","        features = data.get(\"features\", data)\n","        logger.info(f\"üì• Received: {features}\")\n","\n","        try:\n","            result = predict(features)\n","            output = {\n","                \"input\": features,\n","                \"prediction\": result[\"prediction\"],\n","                \"fraud_probability\": result[\"fraud_probability\"]\n","            }\n","            producer.send(PREDICTIONS_TOPIC, output)\n","            logger.info(f\"üì§ Sent prediction: {output}\")\n","        except Exception as e:\n","            logger.error(f\"‚ùå Prediction failed: {e}\")\n","except KeyboardInterrupt:\n","    logger.warning(\"üõë Worker stopped manually.\")\n","finally:\n","    consumer.close()\n","    producer.flush()\n"],"metadata":{"id":"HzDgBPB083Rz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 4: Containerizing the Application with Docker**\n","\n","**Story:**\n","\n","To make the API easy to deploy and scale, you containerize it using Docker. You create a Dockerfile that sets up the environment, installs dependencies, and runs the FastAPI application.\n","\n","**Mini Tasks:**\n","\n","\n","Create a Dockerfile that:\n","Uses a base Python image (e.g., python:3.9-slim).\n","Installs dependencies from a requirements.txt file.\n","Copies the FastAPI application and model file into the container.\n","Exposes the API on port 8000.\n","Build the Docker image and run it locally.\n","Test the API inside the Docker container using sample transaction data."],"metadata":{"id":"96-6OjaohLaD"}},{"cell_type":"markdown","source":["**Dockerfile**"],"metadata":{"id":"meTXv5y3iCuT"}},{"cell_type":"code","source":["# Use official Python image\n","FROM python:3.10-slim\n","\n","# Set work directory\n","WORKDIR /app\n","\n","# Install dependencies\n","COPY requirements.txt .\n","RUN pip install --no-cache-dir -r requirements.txt\n","\n","# Copy app\n","COPY . .\n","\n","# Expose port\n","EXPOSE 8000\n","\n","# Default command (for API container)\n","CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"],"metadata":{"id":"sKTDOh-XejeG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**docker-compose.yml**"],"metadata":{"id":"L2HYHgKJiIya"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5sA1RxJdvTj"},"outputs":[],"source":["services:\n","  zookeeper:\n","    image: confluentinc/cp-zookeeper:7.5.0\n","    ports:\n","      - \"2181:2181\"\n","    environment:\n","      ZOOKEEPER_CLIENT_PORT: 2181\n","      ZOOKEEPER_TICK_TIME: 2000\n","    volumes:\n","      - zookeeper-data:/var/lib/zookeeper/data\n","    healthcheck:\n","      test: [\"CMD-SHELL\", \"echo ruok | nc localhost 2181\"]\n","      interval: 10s\n","      timeout: 5s\n","      retries: 5\n","\n","  kafka:\n","    image: confluentinc/cp-kafka:7.5.0\n","    ports:\n","      - \"9092:9092\"\n","    environment:\n","      KAFKA_BROKER_ID: 1\n","      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n","      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,DOCKER://kafka:29092\n","      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,DOCKER://0.0.0.0:29092\n","      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,DOCKER:PLAINTEXT\n","      KAFKA_INTER_BROKER_LISTENER_NAME: DOCKER\n","      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n","      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n","      # ‚úÖ Reduce verbosity to WARN level\n","      KAFKA_LOG4J_ROOT_LOGLEVEL: \"WARN\"\n","      KAFKA_LOG4J_LOGGERS: \"kafka.controller=WARN,kafka.producer.async.DefaultEventHandler=WARN,state.change.logger=WARN,kafka.request.logger=WARN\"\n","    depends_on:\n","      zookeeper:\n","        condition: service_healthy\n","    volumes:\n","      - kafka-data:/var/lib/kafka/data\n","    healthcheck:\n","      test: [\"CMD\", \"kafka-topics\", \"--bootstrap-server\", \"localhost:9092\", \"--list\"]\n","      interval: 10s\n","      timeout: 10s\n","      retries: 10\n","\n","  topic-init:\n","    build:\n","      context: .\n","      dockerfile: Dockerfile\n","    command: [\"python\", \"scripts/create_topics.py\"]\n","    depends_on:\n","      kafka:\n","        condition: service_healthy\n","    environment:\n","      KAFKA_BROKER: kafka:29092\n","    volumes:\n","      - .:/app\n","    restart: \"no\"\n","\n","  api:\n","    build: .\n","    container_name: fraud_api\n","    ports:\n","      - \"8000:8000\"\n","    depends_on:\n","      topic-init:\n","        condition: service_completed_successfully\n","    environment:\n","      KAFKA_BROKER: kafka:29092\n","      MODEL_PATH: app/model/logistic_regression_model.joblib\n","    command: [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n","    volumes:\n","      - .:/app\n","    restart: unless-stopped\n","    healthcheck:\n","      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/docs\"]\n","      interval: 10s\n","      timeout: 5s\n","      retries: 5\n","    labels:\n","      - \"prometheus.scrape=true\"\n","      - \"prometheus.port=8000\"\n","\n","  worker:\n","    build: .\n","    container_name: fraud_worker\n","    depends_on:\n","      topic-init:\n","        condition: service_completed_successfully\n","    environment:\n","      KAFKA_BROKER: kafka:29092\n","      MODEL_PATH: app/model/logistic_regression_model.joblib\n","    command: [\"python\", \"app/services/kafka_worker.py\"]\n","    volumes:\n","      - .:/app\n","    restart: unless-stopped\n","\n","  producer:\n","    build: .\n","    container_name: kafka_producer\n","    depends_on:\n","      kafka:\n","        condition: service_healthy\n","    environment:\n","      KAFKA_BROKER: kafka:29092\n","    command: [\"python\", \"scripts/kafka_producer.py\"]\n","    volumes:\n","      - .:/app\n","    restart: \"no\"\n","\n","  consumer:\n","    build: .\n","    container_name: kafka_consumer\n","    depends_on:\n","      kafka:\n","        condition: service_healthy\n","    environment:\n","      KAFKA_BROKER: kafka:29092\n","    command: [\"python\", \"scripts/kafka_consumer.py\"]\n","    volumes:\n","      - .:/app\n","    restart: \"no\"\n","\n","  api-tester:\n","    build: .\n","    depends_on:\n","      api:\n","        condition: service_healthy\n","    command: [\"python\", \"scripts/api_stream_test.py\"]\n","    volumes:\n","      - .:/app\n","    restart: \"no\"\n","\n","  prometheus:\n","    image: prom/prometheus:latest\n","    container_name: prometheus\n","    ports:\n","      - \"9090:9090\"\n","    volumes:\n","      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n","    depends_on:\n","      - api\n","    restart: unless-stopped\n","\n","  grafana:\n","    image: grafana/grafana:latest\n","    container_name: grafana\n","    ports:\n","      - \"3000:3000\"\n","    volumes:\n","      - grafana-data:/var/lib/grafana\n","    depends_on:\n","      - prometheus\n","    restart: unless-stopped\n","\n","volumes:\n","  kafka-data:\n","  zookeeper-data:\n","  grafana-data:\n"]},{"cell_type":"markdown","source":["**Step 5: Setting Up Real-Time Streaming with Kafka**\n","\n","**Story:**\n","\n","To enable real-time predictions, you set up Kafka to stream transaction data to the API. The API consumes messages from a Kafka topic, makes predictions, and writes the results to another Kafka topic.\n","\n","**Mini Tasks:**\n","\n","\n","Create two Kafka topics: transactions (for incoming transaction data) and predictions (for prediction results).\n","Modify the FastAPI application to include a Kafka consumer and producer:\n","The consumer reads transaction data from the transactions topic.\n","The producer writes predictions to the predictions topic.\n","Test the real-time streaming setup by producing sample transaction data to the transactions topic and consuming predictions from the predictions topic."],"metadata":{"id":"AQcWMwJriaPy"}},{"cell_type":"markdown","source":["**create_topics.py**"],"metadata":{"id":"9Gi680CCirCT"}},{"cell_type":"code","source":["# scripts/create_topics.py\n","\n","from kafka.admin import KafkaAdminClient, NewTopic\n","from kafka.errors import KafkaError\n","import logging\n","import os\n","import time\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(\"KafkaTopicSetup\")\n","\n","KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"kafka:29092\")\n","TOPICS = [\"transactions\", \"predictions\"]\n","\n","MAX_RETRIES = 5\n","RETRY_DELAY = 5  # seconds\n","\n","def create_topics():\n","    for attempt in range(1, MAX_RETRIES + 1):\n","        try:\n","            admin = KafkaAdminClient(\n","                bootstrap_servers=KAFKA_BROKER,\n","                client_id=\"fraud_detection_admin\"\n","            )\n","\n","            existing_topics = admin.list_topics()\n","            topics_to_create = [\n","                NewTopic(name=topic, num_partitions=1, replication_factor=1)\n","                for topic in TOPICS if topic not in existing_topics\n","            ]\n","\n","            if topics_to_create:\n","                admin.create_topics(new_topics=topics_to_create)\n","                logger.info(f\"[+] Created topics: {[t.name for t in topics_to_create]}\")\n","            else:\n","                logger.info(\"[‚úì] All required topics already exist.\")\n","            break  # Success, exit loop\n","\n","        except KafkaError as e:\n","            logger.warning(f\"[!] Kafka not ready yet (attempt {attempt}): {e}\")\n","            time.sleep(RETRY_DELAY)\n","        except Exception as e:\n","            logger.error(f\"[!] Unexpected error: {e}\")\n","            break\n","\n","if __name__ == \"__main__\":\n","    create_topics()\n"],"metadata":{"id":"cRiTzXC2hSBK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**kafka_producer.py**"],"metadata":{"id":"ddCyQBs3i2Dk"}},{"cell_type":"code","source":["from confluent_kafka import Producer\n","import json\n","\n","producer_conf = {\n","    'bootstrap.servers': 'localhost:9092'\n","}\n","producer = Producer(producer_conf)\n","\n","sample_transaction = {\n","    \"Time\": 0.0,\n","    \"V1\": -1.36, \"V2\": -0.07, \"V3\": 2.53, \"V4\": 1.37, \"V5\": -0.33,\n","    \"V6\": 0.46, \"V7\": 0.23, \"V8\": 0.09, \"V9\": 0.36,\n","    \"V10\": 0.09, \"V11\": -0.55, \"V12\": -0.61, \"V13\": -0.99, \"V14\": -0.31,\n","    \"V15\": 1.46, \"V16\": -0.47, \"V17\": 0.20, \"V18\": 0.02, \"V19\": 0.40,\n","    \"V20\": 0.25, \"V21\": -0.01, \"V22\": 0.27, \"V23\": -0.11, \"V24\": 0.06,\n","    \"V25\": 0.12, \"V26\": -0.18, \"V27\": 0.13, \"V28\": -0.02,\n","    \"Amount\": 149.62\n","}\n","\n","def delivery_report(err, msg):\n","    if err:\n","        print(f\"[!] Delivery failed: {err}\")\n","    else:\n","        print(f\"[‚úì] Message delivered to {msg.topic()} [{msg.partition()}]\")\n","\n","producer.produce(\n","    topic=\"transactions\",\n","    key=\"txn-1\",\n","    value=json.dumps(sample_transaction),\n","    callback=delivery_report\n",")\n","\n","producer.flush()\n"],"metadata":{"id":"pGA-0WJLithq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**kafka_consumer.py**"],"metadata":{"id":"BCIGRgSFjkOK"}},{"cell_type":"code","source":["from confluent_kafka import Consumer\n","import json\n","\n","consumer_conf = {\n","    'bootstrap.servers': 'localhost:9092',\n","    'group.id': 'prediction-group',\n","    'auto.offset.reset': 'earliest'\n","}\n","consumer = Consumer(consumer_conf)\n","consumer.subscribe([\"predictions\"])\n","\n","print(\"[*] Waiting for prediction messages...\\n\")\n","\n","try:\n","    while True:\n","        msg = consumer.poll(1.0)\n","        if msg is None:\n","            continue\n","        if msg.error():\n","            print(f\"[!] Error: {msg.error()}\")\n","            continue\n","\n","        prediction = json.loads(msg.value().decode(\"utf-8\"))\n","        print(\"[‚úì] Prediction received:\")\n","        print(json.dumps(prediction, indent=4))\n","\n","except KeyboardInterrupt:\n","    print(\"\\n[!] Stopped by user.\")\n","finally:\n","    consumer.close()\n"],"metadata":{"id":"wQbizZF5jo_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Step 6: Deploying the Application\n","\n","Story:\n","\n","You deploy the Dockerized FastAPI application and Kafka setup to a cloud platform (e.g., AWS, Azure, or GCP) or run it locally for testing. You ensure that the API and Kafka are working together seamlessly to provide real-time predictions.\n","\n","\n","Mini Tasks:\n","\n","Push the Docker image to a container registry (e.g., Docker Hub, Azure Container Registry).\n","Deploy the Docker container to a cloud service (e.g., AWS ECS, Azure Container Instances) or run it locally using Docker Compose.\n","Verify that the API and Kafka are working together by streaming live transaction data and checking the predictions."],"metadata":{"id":"Xgk4NMxxk90z"}},{"cell_type":"markdown","source":["**Build & Run All Services**"],"metadata":{"id":"MY5TQWK4lAeL"}},{"cell_type":"markdown","source":["### ‚úÖ Option A: Local Deployment with Docker Compose"],"metadata":{"id":"Y8IWtGynmXFy"}},{"cell_type":"code","source":["# üîß 1. Build & Run All Services\n","\n","docker-compose up --build\n","\n","\n","# üß™ 2. Verify the Deployment\n","# API Health Check:\n","\n","Visit: http://localhost:8000/docs\n","\n","# Grafana Dashboard:\n","\n","Visit: http://localhost:3000\n","\n","\n","# Prometheus:\n","\n","Visit: http://localhost:9090\n","\n","\n","# Use queries like:\n","\n","http_requests_total\n","\n","\n","# Streaming Test:\n","\n","docker-compose run api-tester"],"metadata":{"id":"caMANV51k_3M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ‚úÖ Option B: Cloud Deployment (AWS)"],"metadata":{"id":"tosS4vjNmk1b"}},{"cell_type":"code","source":["## ‚úÖ Prerequisites\n","\n","* AWS account\n","* AWS CLI installed and configured: `aws configure`\n","* Docker installed\n","* Docker Hub account\n","* Docker images for all services (API, worker, producer, consumer, tester) pushed to Docker Hub\n","\n","## ‚ë† Push Docker Images to Docker Hub\n","\n","docker login\n","\n","# Push API\n","docker tag fraud_api yourdockerhub/fraud_api:latest\n","docker push yourdockerhub/fraud_api:latest\n","\n","# Push Worker\n","docker tag fraud_worker yourdockerhub/fraud_worker:latest\n","docker push yourdockerhub/fraud_worker:latest\n","\n","# Push Kafka Producer\n","docker tag kafka_producer yourdockerhub/kafka_producer:latest\n","docker push yourdockerhub/kafka_producer:latest\n","\n","# Push Kafka Consumer\n","docker tag kafka_consumer yourdockerhub/kafka_consumer:latest\n","docker push yourdockerhub/kafka_consumer:latest\n","\n","# Push API Tester\n","docker tag api_tester yourdockerhub/api_tester:latest\n","docker push yourdockerhub/api_tester:latest\n","\n","## ‚ë° Create an ECS Cluster\n","\n","aws ecs create-cluster --cluster-name fraud-detection-cluster\n","\n","## ‚ë¢ Create IAM Role for ECS Task Execution\n","\n","1. Create the trust policy JSON file:\n","\n","```json\n","{\n","  \"Version\": \"2012-10-17\",\n","  \"Statement\": [\n","    {\n","      \"Effect\": \"Allow\",\n","      \"Principal\": {\n","        \"Service\": \"ecs-tasks.amazonaws.com\"\n","      },\n","      \"Action\": \"sts:AssumeRole\"\n","    }\n","  ]\n","}\n","\n","Save it as `ecs-trust-policy.json`\n","\n","2. Create the IAM role and attach permissions:\n","\n","aws iam create-role \\\n","  --role-name ecsTaskExecutionRole \\\n","  --assume-role-policy-document file://ecs-trust-policy.json\n","\n","aws iam attach-role-policy \\\n","  --role-name ecsTaskExecutionRole \\\n","  --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\n","\n","## ‚ë£ Create Task Definitions for Each Service\n","\n","Use a similar format for each container. Here's an example for the API:\n","\n","**`fraud-api-task.json`**\n","\n","```json\n","{\n","  \"family\": \"fraud-api-task\",\n","  \"networkMode\": \"awsvpc\",\n","  \"requiresCompatibilities\": [\"FARGATE\"],\n","  \"cpu\": \"256\",\n","  \"memory\": \"512\",\n","  \"executionRoleArn\": \"arn:aws:iam::<your_account_id>:role/ecsTaskExecutionRole\",\n","  \"containerDefinitions\": [\n","    {\n","      \"name\": \"fraud_api\",\n","      \"image\": \"yourdockerhub/fraud_api:latest\",\n","      \"portMappings\": [\n","        {\n","          \"containerPort\": 8000,\n","          \"protocol\": \"tcp\"\n","        }\n","      ],\n","      \"essential\": true,\n","      \"environment\": [\n","        {\n","          \"name\": \"KAFKA_BROKER\",\n","          \"value\": \"<your-msk-bootstrap-server>:9092\"\n","        },\n","        {\n","          \"name\": \"MODEL_PATH\",\n","          \"value\": \"app/model/logistic_regression_model.joblib\"\n","        }\n","      ]\n","    }\n","  ]\n","}\n","\n","\n","Register the task:\n","\n","\n","aws ecs register-task-definition \\\n","  --cli-input-json file://fraud-api-task.json\n","\n","\n","Repeat this for:\n","\n","* `fraud_worker_task.json`\n","* `kafka_producer_task.json`\n","* `kafka_consumer_task.json`\n","* `api_tester_task.json`\n","\n","## ‚ë§ Set Up Networking (VPC, Subnets, Security Groups)\n","\n","Use the AWS Console or CLI to create:\n","\n","* A new VPC with subnets in at least 2 AZs\n","* An Internet Gateway attached to the VPC\n","* Route table with access to the internet\n","* Security group allowing HTTP (port 80/8000) and internal Kafka ports\n","\n","## ‚ë• Deploy Each Task to Fargate\n","\n","### One-time Task (e.g., Worker, Producer)\n","\n","aws ecs run-task \\\n","  --cluster fraud-detection-cluster \\\n","  --launch-type FARGATE \\\n","  --network-configuration 'awsvpcConfiguration={subnets=[subnet-abc123],securityGroups=[sg-abc123],assignPublicIp=\"ENABLED\"}' \\\n","  --task-definition fraud-worker-task\n","\n","### Long-running Services (e.g., API)\n","\n","aws ecs create-service \\\n","  --cluster fraud-detection-cluster \\\n","  --service-name fraud-api-service \\\n","  --task-definition fraud-api-task \\\n","  --desired-count 1 \\\n","  --launch-type FARGATE \\\n","  --network-configuration 'awsvpcConfiguration={subnets=[subnet-abc123],securityGroups=[sg-abc123],assignPublicIp=\"ENABLED\"}'\n","\n","Repeat for other services like the Kafka producer or consumer if long-running.\n","\n","## ‚ë¶ Optional: Load Balancer for API\n","\n","Use an **Application Load Balancer (ALB)** to expose the FastAPI service to the internet:\n","\n","* Target group: Fraud API ECS service\n","* Listener: Forward HTTP requests (port 80) to target group\n","* Add DNS record pointing to the ALB\n","\n","## ‚ëß Monitoring and Logs\n","\n","Use **CloudWatch** to monitor logs:\n","\n","aws logs describe-log-groups\n","aws logs get-log-events \\\n","  --log-group-name /ecs/fraud-api-task \\\n","  --log-stream-name <log-stream>\n","\n","## ‚úÖ Final Notes\n","\n","* Use **Amazon MSK** for managed Kafka\n","* Use **AWS Secrets Manager** for credentials\n","\n","\n"],"metadata":{"id":"1X7_4IicmiWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 7: Testing and Monitoring\n","\n","Story:\n","\n","You test the entire system end-to-end to ensure it works as expected. You also set up basic monitoring to track the performance of the API and Kafka.\n","\n","Mini Tasks:\n","\n","\n","Test the system by streaming a large batch of transaction data and verifying the predictions.\n","Set up logging in the FastAPI application to track incoming requests and predictions.\n","Use a tool like Prometheus or Grafana to monitor the API's performance and Kafka's message throughput."],"metadata":{"id":"UlCvOt7Jrv5v"}},{"cell_type":"markdown","source":["**api_stream_test.py**"],"metadata":{"id":"RldCFkogrCR1"}},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","import time\n","\n","# Load sample transaction data\n","data = pd.read_csv(\"data/sample_transactions.csv\")\n","\n","# API endpoint (running inside Docker, localhost works if API is mapped to host port 8000)\n","url = \"http://localhost:8000/predict\"\n","\n","print(f\"üöÄ Starting transaction stream to {url} ...\\n\")\n","\n","for idx, row in data.iterrows():\n","    payload = row.to_dict()\n","\n","    try:\n","        response = requests.post(url, json=payload)\n","        response.raise_for_status()\n","        result = response.json()\n","        print(f\"‚úÖ Transaction {idx + 1}: Prediction = {result}\")\n","    except requests.exceptions.RequestException as e:\n","        print(f\"‚ùå Transaction {idx + 1} failed: {e}\")\n","\n","    time.sleep(0.2)  # Simulate delay between incoming transactions\n"],"metadata":{"id":"kg1luLsCj4X7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**prometheus.yml**"],"metadata":{"id":"VQRus48isYSF"}},{"cell_type":"code","source":["global:\n","  scrape_interval: 15s\n","\n","scrape_configs:\n","  - job_name: 'fraud_api'\n","    static_configs:\n","      - targets: ['api:8000']\n"],"metadata":{"id":"Gq_nWQ7VsSaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Grafana Dashboard Setup\n","Open http://localhost:3000\n","\n","Login with admin/admin\n","\n","Add Prometheus as a data source (http://localhost:9090)\n","\n","Create dashboards:\n","\n","API Response Time\n","\n","Prediction Count"],"metadata":{"id":"FiSZgd-Fs8e_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Conclusion**\n","\n","**Story:**\n","\n","You‚Äôve successfully deployed a machine learning model as a REST API using FastAPI, containerized it with Docker, and set up real-time streaming for live predictions using Kafka. This system can now be integrated into your live transaction processing pipeline to detect fraudulent transactions in real time.\n","\n","\n","**Mini Tasks:**\n","\n","Document the setup process and share it with your team.\n","Terminate any cloud resources (if used) to avoid unnecessary costs."],"metadata":{"id":"emE2mQDy1nJP"}},{"cell_type":"markdown","source":["# Real-Time Fraud Detection System\n","\n","## üöÄ Overview\n","\n","A machine learning-powered real-time fraud detection system leveraging FastAPI, Kafka, Docker, and monitoring tools (Prometheus & Grafana). This project simulates live transaction streaming, prediction processing, and real-time monitoring.\n","\n","## üìÅ Project Structure\n","\n","```\n","‚îú‚îÄ‚îÄ app/\n","‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/                  # Python bytecode cache\n","‚îÇ   ‚îú‚îÄ‚îÄ data/\n","‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sample_transactions.csv   # Example data for testing\n","‚îÇ   ‚îú‚îÄ‚îÄ model/\n","‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logistic_regression_model.joblib\n","‚îÇ   ‚îú‚îÄ‚îÄ services/\n","‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kafka_worker.py\n","‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                   # FastAPI app\n","‚îú‚îÄ‚îÄ scripts/\n","‚îÇ   ‚îú‚îÄ‚îÄ api_stream_test.py           # Script to test API stream\n","‚îÇ   ‚îú‚îÄ‚îÄ create_topics.py             # Kafka topic creation\n","‚îÇ   ‚îú‚îÄ‚îÄ kafka_consumer.py            # Kafka consumer logic\n","‚îÇ   ‚îî‚îÄ‚îÄ kafka_producer.py            # Kafka producer logic\n","‚îú‚îÄ‚îÄ venv/                            # Python virtual environment\n","‚îú‚îÄ‚îÄ prometheus.yml                   # Monitoring config\n","‚îú‚îÄ‚îÄ docker-compose.yml              # Multi-container setup\n","‚îú‚îÄ‚îÄ Dockerfile                      # App container definition\n","‚îú‚îÄ‚îÄ README.md\n","‚îî‚îÄ‚îÄ requirements.txt\n","\n","```\n","\n","## üì¶ Environment Setup\n","\n","### Required Tools\n","\n","* Python 3.10+\n","* Docker & Docker Compose\n","* Kafka\n","* Prometheus & Grafana\n","\n","### Python Dependencies\n","\n","```bash\n","pip install fastapi uvicorn[standard] scikit-learn joblib pandas numpy kafka-python confluent-kafka prometheus-fastapi-instrumentator\n","```\n","\n","### Docker Compose Setup\n","\n","Includes services:\n","\n","* `zookeeper`, `kafka`\n","* `api`, `worker`, `producer`, `consumer`\n","* `prometheus`, `grafana`\n","\n","\n","## üß† Model Development\n","\n","* **Dataset**: [Credit Card Fraud - Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n","* **Preprocessing**:\n","\n","  * Handle imbalance using SMOTE\n","  * Scale `Amount` with `RobustScaler`\n","  * Use `PowerTransformer` to reduce skew\n","* **Model**: Logistic Regression with `StandardScaler`, `SelectKBest`, `GridSearchCV`\n","* **Evaluation**: Confusion Matrix, ROC AUC, Precision, etc.\n","\n","### Save Trained Model\n","\n","```python\n","joblib.dump(model, \"logistic_regression_model.joblib\")\n","```\n","\n","## üåê REST API (FastAPI)\n","\n","### Endpoints\n","\n","* `GET /` ‚Üí Welcome\n","* `GET /health` ‚Üí Health check\n","* `POST /predict` ‚Üí Predict fraud\n","\n","### Features\n","\n","* Input validated using Pydantic\n","* Model loaded once at startup\n","* Returns fraud label + probability\n","* Prometheus metrics auto-exposed\n","\n","### Sample Input\n","\n","```json\n","{\n","  \"Time\": 0.0,\n","  \"V1\": -1.36,\n","  \"V2\": 0.57,\n","  ...\n","  \"V28\": -0.02,\n","  \"Amount\": 149.62\n","}\n","```\n","\n","## üê≥ Docker Setup\n","\n","### Dockerfile\n","\n","```dockerfile\n","FROM python:3.10-slim\n","WORKDIR /app\n","COPY requirements.txt .\n","RUN pip install -r requirements.txt\n","COPY . .\n","EXPOSE 8000\n","CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n","```\n","\n","### docker-compose.yml Highlights\n","\n","* Health checks\n","* Volumes for persistence\n","* Inter-service networking\n","\n","\n","## üîÅ Kafka Streaming\n","\n","### Topics\n","\n","* `transactions` ‚Üí Input stream\n","* `predictions` ‚Üí Output results\n","\n","### Kafka Roles\n","\n","* **Producer**: Streams new transactions\n","* **Worker**: Subscribes to `transactions`, predicts, publishes to `predictions`\n","* **Consumer**: Reads `predictions` for logging/UI\n","\n","### Key Scripts\n","\n","```bash\n","python kafka/create_topics.py\n","python kafka/kafka_producer.py\n","python kafka/kafka_worker.py\n","python kafka/kafka_consumer.py\n","```\n","\n","\n","## ‚òÅÔ∏è Deployment Options\n","\n","### Option A: Local Deployment\n","\n","```bash\n","docker-compose up --build\n","```\n","\n","Access:\n","\n","* FastAPI Swagger: [http://localhost:8000/docs](http://localhost:8000/docs)\n","* Prometheus: [http://localhost:9090](http://localhost:9090)\n","* Grafana: [http://localhost:3000](http://localhost:3000) (admin/admin)\n","\n","### Option B: AWS ECS Fargate (Optional)\n","\n","1. Push Docker images to Docker Hub\n","2. Create ECS cluster, VPC, roles\n","3. Define ECS Task Definitions per service\n","4. Setup Application Load Balancer\n","5. Deploy & monitor with CloudWatch\n","\n","\n","## üß™ Testing and Monitoring\n","\n","### Test Transactions\n","\n","* Use `api_stream_test.py` to stream CSV transactions to `/predict`\n","\n","### Logging\n","\n","* Python `logging` captures API requests & predictions\n","\n","### Prometheus\n","\n","```yaml\n","scrape_configs:\n","  - job_name: 'fraud_api'\n","    static_configs:\n","      - targets: ['api:8000']\n","```\n","\n","### Grafana\n","\n","* Add Prometheus as Data Source\n","* Create Dashboard with:\n","\n","  * Request Count\n","  * Fraud Rate\n","  * Response Latency\n","\n","\n","## ‚úÖ Final Notes\n","\n","* Scalable, real-time architecture for fraud detection\n","* Plug-and-play with different models\n","* Full observability with monitoring tools\n","\n","\n","## üôè Credits\n","\n","* [Kaggle Credit Card Fraud Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n","* FastAPI, Docker, Kafka, Prometheus, Grafana\n","\n","\n","Happy detecting! üéØ\n"],"metadata":{"id":"yR1jyWlPO-Gj"}}]}